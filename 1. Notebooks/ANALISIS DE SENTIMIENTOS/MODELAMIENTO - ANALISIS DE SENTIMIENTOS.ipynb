{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKPRako4cqVr1DNkJvnNEw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["6.1 Procesamiento de texto"],"metadata":{"id":"unqUfiiqhFPk"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"id":"oOV7q3t5fk3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['text'] = bd['text'].astype(str)"],"metadata":{"id":"5K-c_-xifmnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instanciar el tokenizador de tweets\n","tokenizer = TweetTokenizer()\n","\n","# Lista para almacenar todos los tokens\n","all_tokens = []\n","\n","# Iterar sobre todas las columnas del DataFrame\n","\n","for text in bd['text']:\n","    tokens = tokenizer.tokenize(text)\n","    all_tokens.extend(tokens)\n","\n","print(\"all_tokens_title =\",len(all_tokens))"],"metadata":{"id":"tI005OE8foRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analisis de frecuencia en palabras\n","fdist = nltk.FreqDist(all_tokens)\n","print('Size BoW=',len(fdist))\n","topwords = fdist.most_common(20)"],"metadata":{"id":"QfdsmOALfpVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"-lYXjbK1fqda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stopwords en nltk\n","from nltk.corpus import stopwords\n","\n","stop_words_nltk = set(stopwords.words('spanish'))"],"metadata":{"id":"1qAdKvkAfsBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","#  TOKENIZAR con nltk,\n","# ELIMINAR tokens de long = 1\n","# ELIMINAR caracteres que no sean alfanumericos\n","# REMOVER stop words\n","# graficar los 20 t칠rminos m치s frecuentes:\n","\n","# ya tokenizado en all_tokens\n","tokens = [w.lower() for w in all_tokens if len(w)>1]\n","tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n","tokens = [w for w in tokens if w not in stop_words_nltk]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('Size of new BoW=',len(fdist))\n","x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"JH4ouxECfs45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Stemming con NLTK\n","\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","\n","\n","tokens = [lancaster.stem(w) for w in tokens]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('Size of new BoW =',len(fdist))\n"],"metadata":{"id":"XzWo3XhNft_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lemmatization con NLTK\n","import nltk\n","nltk.download('wordnet')\n","\n","from nltk.stem import WordNetLemmatizer\n","\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('Size of new BoW =',len(fdist))"],"metadata":{"id":"kEsdAk7nfu-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","vectorizer = TfidfVectorizer()\n","\n","# Ajustar y transformar los documentos en una matriz TF-IDF\n","tfidf_matrix = vectorizer.fit_transform(tokens)\n","\n","# Convertir las columnas de la matriz dispersa en un DataFrame disperso\n","df_tfidf_sparse = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n","\n","\n","\n","\n","# Mostrar el DataFr\n","\n","print(df_tfidf_sparse)"],"metadata":{"id":"CQ6VlGigfv8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words = set(stopwords.words('spanish'))\n","\n","# Funci칩n de limpieza de texto\n","def limpiar_texto(text):\n","    text = re.sub(r'http\\S+', '', text)  # Eliminar URLs\n","    text = re.sub(r'@\\w+', '', text)     # Eliminar menciones\n","    text = re.sub(r'#\\w+', '', text)     # Eliminar hashtags\n","    text = re.sub(r'\\d+', '', text)      # Eliminar n칰meros\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar puntuaciones\n","    text = text.lower()                  # Convertir a min칰sculas\n","    text = ' '.join([word for word in text.split() if word not in stop_words])\n","    return text\n","\n","# Aplicar la limpieza a la columna de tuits\n","bd['texto_limpio'] = bd['text'].apply(limpiar_texto)"],"metadata":{"id":"_PpGP0ARfxRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['texto_limpio']"],"metadata":{"id":"swxHrifwfyIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Funci칩n para obtener el sentimiento\n","def obtener_sentimiento(text):\n","    analisis = TextBlob(text)\n","    if analisis.sentiment.polarity > 0:\n","        return 'positivo'\n","    elif analisis.sentiment.polarity == 0:\n","        return 'neutral'\n","    else:\n","        return 'negativo'\n","\n","# Aplicar el an치lisis de sentimiento en la columna limpia\n","bd['sentimento'] = bd['texto_limpio'].apply(obtener_sentimiento)"],"metadata":{"id":"5-1PedR-fzMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['sentimento'].value_counts()"],"metadata":{"id":"VXfLpC4Mf0Cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Convertir el texto en una matriz de cuentas (CountVectorizer)\n","count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","X = count_vectorizer.fit_transform(bd['texto_limpio'])\n","\n","# Aplicar LDA para identificar los t칩picos\n","lda = LatentDirichletAllocation(n_components=5, random_state=42)\n","lda.fit(X)\n","\n","# Mostrar los t칠rminos principales de cada t칩pico\n","def mostrar_topicos(modelo, count_vectorizer, n_palabras):\n","    palabras = count_vectorizer.get_feature_names_out()\n","    for idx, topico in enumerate(modelo.components_):\n","        print(f\"T칩pico {idx}:\")\n","        print([palabras[i] for i in topico.argsort()[-n_palabras:]])\n","\n","mostrar_topicos(lda, count_vectorizer, 10)"],"metadata":{"id":"XUtkS6Wvf09K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pipeline"],"metadata":{"id":"gSWoZOVuf18S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Cargar el pipeline de an치lisis de emociones\n","clasificador = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base')\n","\n","# Analizar emociones en textos\n","bd['emociones'] = bd['text'].apply(lambda x: clasificador(x))"],"metadata":{"id":"ohcBj9Clf2xy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# Cargar el modelo SBERT\n","modelo = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Dataset de texto\n","texto = [\"Estoy muy feliz\", \"Esto es muy frustrante\", \"Me siento triste\"]\n","\n","# Definir emociones base\n","emociones_base = {\n","    \"alegr칤a\": \"Esto me hace muy feliz y emocionado.\",\n","    \"tristeza\": \"Siento una profunda tristeza y desolaci칩n.\",\n","    \"enojo\": \"Estoy molesto y frustrado.\"\n","}\n","\n","# Generar embeddings para textos y emociones base\n","emb_textos = modelo.encode(texto)\n","emb_emociones = modelo.encode(list(emociones_base.values()))\n","\n","# Calcular similitud\n","for i, t in enumerate(texto):\n","    similitudes = cosine_similarity([emb_textos[i]], emb_emociones)\n","    emocion_predicha = list(emociones_base.keys())[np.argmax(similitudes)]\n","    print(f\"Texto: {t}\\nEmoci칩n: {emocion_predicha}\\n\")"],"metadata":{"id":"nvYIk1rFf4Aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# Cargar el modelo SBERT\n","modelo = SentenceTransformer(\"all-MiniLM-L6-v2\")"],"metadata":{"id":"b95CPu48f5Gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto = bd['text'].tolist()"],"metadata":{"id":"WRsckzGsf5xZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definir emociones base\n","emociones_base = {\n","    \"alegr칤a\": \"Esto me hace muy feliz y emocionado.\",\n","    \"tristeza\": \"Siento una profunda tristeza y desolaci칩n.\",\n","    \"enojo\": \"Estoy molesto y frustrado.\"\n","}\n","\n","# Generar embeddings para textos y emociones base\n","emb_textos = modelo.encode(texto)\n","emb_emociones = modelo.encode(list(emociones_base.values()))\n","\n","# Calcular similitud\n","for i, t in enumerate(texto):\n","    similitudes = cosine_similarity([emb_textos[i]], emb_emociones)\n","    emocion_predicha = list(emociones_base.keys())[np.argmax(similitudes)]\n","    print(f\"Texto: {t}\\nEmoci칩n: {emocion_predicha}\\n\")"],"metadata":{"id":"ocoNQvGnf6yC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.cluster import KMeans\n","import pandas as pd\n","\n","# Cargar el modelo LaBSE\n","modelo = SentenceTransformer(\"sentence-transformers/LaBSE\")\n","\n","# Generar vectores\n","vectores = modelo.encode(bd['text'].tolist())\n","\n","# Agrupar con KMeans\n","num_clusters = 3\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n","clusters = kmeans.fit_predict(vectores)\n","\n","# A침adir resultados al DataFrame\n","bd[\"cluster\"] = clusters"],"metadata":{"id":"fIsZs7sFf8FJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd.head(10)"],"metadata":{"id":"f3qHW3Lzf9IC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['cluster'].value_counts()"],"metadata":{"id":"YfXxWpVwf-Oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funci칩n de limpieza de texto\n","def limpiar_texto(text):\n","    text = re.sub(r'http\\S+', '', text)  # Eliminar URLs\n","    text = re.sub(r'@\\w+', '', text)     # Eliminar menciones\n","    text = re.sub(r'#\\w+', '', text)     # Eliminar hashtags\n","    text = re.sub(r'\\d+', '', text)      # Eliminar n칰meros\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar puntuaciones\n","    text = text.lower()                  # Convertir a min칰sculas\n","    return text\n","\n","# Aplicar la limpieza a la columna de tuits\n","bd['texto_limpio2'] = bd['text'].apply(limpiar_texto)"],"metadata":{"id":"BqO5WqQ0f_W6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto = bd['texto_limpio2'].tolist()"],"metadata":{"id":"BB3lDSxsgAHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Modelo preentrenado SBERT"],"metadata":{"id":"-R6r1LxtgBD5"}},{"cell_type":"code","source":["# Definir emociones base\n","emociones_base = {\n","    \"positivo\": ['Que estilo', 'La tienda m치s cool del mundo', 'Me merecen esos jogger',  'lo hacen muy bien', 'bonita la ropa', 'me encanta su contenido', 'La mejor ropa','Belleza', 'Wow 游땘仇벒잺', 'Me encanta el 3','칔ltimamente me gusta sus prendas ','Eso si es verdad, excelente calidad en las telas, amo las camisas blancas de ustedes, tengo 3 ','La calidad de las telas si ha mejorado much칤simo. 游땘游땘游땘游땘游땘','Amo ese vestido blanco lo veo y lo quiero 游땘'],\n","    \"neutro\": ['precio por favor',  'el top est치 en la tienda','item de la camiseta', 'lo tienen en laureles medell칤n talla s' , 'por el sitio web se pueden comprar?',\n","               'donde est치n ubicados', 'que item tiene el pantalon'],\n","    \"negativo\": ['Hagan tallas m치s peque침as',  'la otra vez  no hab칤an', 'Jam치s los dejan entrar', 'Traten de que hagan talla grandes', 'la ropa esta cara',\n","                  'la pagina no deja comprar', 'la ropa esta fea','Buenas tardes realice un pedido y me lo enviaron por TCC pero mirando la gu칤a dice que llevan 2 intentos y no encuentran mi direcci칩n',\n","                  'Cuando algo que no sea oversize ? 游땩','Se les acabaron las ideas? 游봆','saquen otra vez vestiditos sueltos como los de hace a침os ,hacen falta 游땩',\n","                  'Tuve un problema con una compra me gustar칤a resolverlo',\n","                  'Creo que sus camisetas y sus 칰ltimas publicaciones acerca de la \"friendzone\" atentan terriblemente contra el derecho de decir NO de una mujer']\n","\n","}\n","\n","# Generar embeddings para emociones base\n","emociones_textos = [item for sublist in emociones_base.values() for item in sublist]  # Aplanar la lista de emociones\n","emb_emociones = modelo.encode(emociones_textos)\n","\n","# Crear un diccionario de emociones con 칤ndices\n","emocion_indices = list(emociones_base.keys())\n","emocion_dict = {}\n","index = 0\n","for emocion, frases in emociones_base.items():\n","    emocion_dict[emocion] = list(range(index, index + len(frases)))\n","    index += len(frases)\n","\n","def predecir_emocion(texto):\n","    try:\n","        emb_texto = modelo.encode([texto])\n","        similitudes = cosine_similarity(emb_texto, emb_emociones)\n","        max_index = np.argmax(similitudes)  # Obtener el 칤ndice de la m치xima similitud\n","        emocion_predicha = None\n","        for emocion, indices in emocion_dict.items():\n","            if max_index in indices:\n","                emocion_predicha = emocion\n","                break\n","        return emocion_predicha\n","    except Exception as e:\n","        print(f\"Error procesando el texto: {texto}, {e}\")\n","        return None\n","\n","# Aplicar al dataframe\n","bd['emocion'] = bd['texto_limpio2'].apply(predecir_emocion)"],"metadata":{"id":"m3HDyqJBgCyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['emocion'].value_counts()"],"metadata":{"id":"R_UIuRqrgDnp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd[bd['emocion'] == 'negativo'].tail(10)"],"metadata":{"id":"olSAqftMgEdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cargar el modelo LaBSE\n","modelo = SentenceTransformer(\"sentence-transformers/LaBSE\")\n","\n","# Generar vectores\n","vectores = modelo.encode(bd['texto_limpio2'].tolist())\n","\n","# Agrupar con KMeans\n","num_clusters = 3\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n","clusters = kmeans.fit_predict(vectores)\n","\n","# A침adir resultados al DataFrame\n","bd[\"cluster\"] = clusters"],"metadata":{"id":"Q3TkLcYvgFXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd[bd['cluster'] == 0].tail(10)"],"metadata":{"id":"kCENNuhUgGa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funci칩n de limpieza que conserva emojis\n","def limpiar_texto_con_emojis(text):\n","    text = re.sub(r'http\\S+', '', text)  # Eliminar URLs\n","    text = re.sub(r'@\\w+', '', text)     # Eliminar menciones\n","    text = re.sub(r'#\\w+', '', text)     # Eliminar hashtags\n","    text = re.sub(r'\\d+', '', text)      # Eliminar n칰meros\n","    text = re.sub(r'[^\\w\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF]', '', text)\n","    text = text.lower()                  # Convertir a min칰sculas\n","    return text\n","\n","bd['texto_limpio3'] = bd['text'].apply(limpiar_texto)"],"metadata":{"id":"2SCBZxfMgHkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generar vectores\n","vectores = modelo.encode(bd['texto_limpio3'].tolist())\n","\n","# Agrupar con KMeans\n","num_clusters = 3\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n","clusters = kmeans.fit_predict(vectores)\n","\n","# A침adir resultados al DataFrame\n","bd[\"cluster\"] = clusters"],"metadata":{"id":"dJaUy6twgIgJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","\n","# Cargar un modelo compatible\n","modelo_beto_sbert = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n","\n","print(\"Modelo BETO-SBERT alternativo cargado con 칠xito.\")"],"metadata":{"id":"CJp2F3-9gKBS"},"execution_count":null,"outputs":[]}]}