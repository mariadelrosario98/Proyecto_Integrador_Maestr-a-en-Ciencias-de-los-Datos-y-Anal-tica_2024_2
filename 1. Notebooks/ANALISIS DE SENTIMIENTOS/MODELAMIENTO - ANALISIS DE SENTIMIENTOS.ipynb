{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKPRako4cqVr1DNkJvnNEw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["6.1 Procesamiento de texto"],"metadata":{"id":"unqUfiiqhFPk"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"id":"oOV7q3t5fk3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['text'] = bd['text'].astype(str)"],"metadata":{"id":"5K-c_-xifmnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instanciar el tokenizador de tweets\n","tokenizer = TweetTokenizer()\n","\n","# Lista para almacenar todos los tokens\n","all_tokens = []\n","\n","# Iterar sobre todas las columnas del DataFrame\n","\n","for text in bd['text']:\n","    tokens = tokenizer.tokenize(text)\n","    all_tokens.extend(tokens)\n","\n","print(\"all_tokens_title =\",len(all_tokens))"],"metadata":{"id":"tI005OE8foRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analisis de frecuencia en palabras\n","fdist = nltk.FreqDist(all_tokens)\n","print('Size BoW=',len(fdist))\n","topwords = fdist.most_common(20)"],"metadata":{"id":"QfdsmOALfpVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"-lYXjbK1fqda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stopwords en nltk\n","from nltk.corpus import stopwords\n","\n","stop_words_nltk = set(stopwords.words('spanish'))"],"metadata":{"id":"1qAdKvkAfsBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","#  TOKENIZAR con nltk,\n","# ELIMINAR tokens de long = 1\n","# ELIMINAR caracteres que no sean alfanumericos\n","# REMOVER stop words\n","# graficar los 20 términos más frecuentes:\n","\n","# ya tokenizado en all_tokens\n","tokens = [w.lower() for w in all_tokens if len(w)>1]\n","tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n","tokens = [w for w in tokens if w not in stop_words_nltk]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('Size of new BoW=',len(fdist))\n","x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"JH4ouxECfs45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Stemming con NLTK\n","\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","\n","\n","tokens = [lancaster.stem(w) for w in tokens]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('Size of new BoW =',len(fdist))\n"],"metadata":{"id":"XzWo3XhNft_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lemmatization con NLTK\n","import nltk\n","nltk.download('wordnet')\n","\n","from nltk.stem import WordNetLemmatizer\n","\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('Size of new BoW =',len(fdist))"],"metadata":{"id":"kEsdAk7nfu-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","vectorizer = TfidfVectorizer()\n","\n","# Ajustar y transformar los documentos en una matriz TF-IDF\n","tfidf_matrix = vectorizer.fit_transform(tokens)\n","\n","# Convertir las columnas de la matriz dispersa en un DataFrame disperso\n","df_tfidf_sparse = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n","\n","\n","\n","\n","# Mostrar el DataFr\n","\n","print(df_tfidf_sparse)"],"metadata":{"id":"CQ6VlGigfv8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words = set(stopwords.words('spanish'))\n","\n","# Función de limpieza de texto\n","def limpiar_texto(text):\n","    text = re.sub(r'http\\S+', '', text)  # Eliminar URLs\n","    text = re.sub(r'@\\w+', '', text)     # Eliminar menciones\n","    text = re.sub(r'#\\w+', '', text)     # Eliminar hashtags\n","    text = re.sub(r'\\d+', '', text)      # Eliminar números\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar puntuaciones\n","    text = text.lower()                  # Convertir a minúsculas\n","    text = ' '.join([word for word in text.split() if word not in stop_words])\n","    return text\n","\n","# Aplicar la limpieza a la columna de tuits\n","bd['texto_limpio'] = bd['text'].apply(limpiar_texto)"],"metadata":{"id":"_PpGP0ARfxRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['texto_limpio']"],"metadata":{"id":"swxHrifwfyIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Función para obtener el sentimiento\n","def obtener_sentimiento(text):\n","    analisis = TextBlob(text)\n","    if analisis.sentiment.polarity > 0:\n","        return 'positivo'\n","    elif analisis.sentiment.polarity == 0:\n","        return 'neutral'\n","    else:\n","        return 'negativo'\n","\n","# Aplicar el análisis de sentimiento en la columna limpia\n","bd['sentimento'] = bd['texto_limpio'].apply(obtener_sentimiento)"],"metadata":{"id":"5-1PedR-fzMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['sentimento'].value_counts()"],"metadata":{"id":"VXfLpC4Mf0Cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Convertir el texto en una matriz de cuentas (CountVectorizer)\n","count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","X = count_vectorizer.fit_transform(bd['texto_limpio'])\n","\n","# Aplicar LDA para identificar los tópicos\n","lda = LatentDirichletAllocation(n_components=5, random_state=42)\n","lda.fit(X)\n","\n","# Mostrar los términos principales de cada tópico\n","def mostrar_topicos(modelo, count_vectorizer, n_palabras):\n","    palabras = count_vectorizer.get_feature_names_out()\n","    for idx, topico in enumerate(modelo.components_):\n","        print(f\"Tópico {idx}:\")\n","        print([palabras[i] for i in topico.argsort()[-n_palabras:]])\n","\n","mostrar_topicos(lda, count_vectorizer, 10)"],"metadata":{"id":"XUtkS6Wvf09K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pipeline"],"metadata":{"id":"gSWoZOVuf18S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Cargar el pipeline de análisis de emociones\n","clasificador = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base')\n","\n","# Analizar emociones en textos\n","bd['emociones'] = bd['text'].apply(lambda x: clasificador(x))"],"metadata":{"id":"ohcBj9Clf2xy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# Cargar el modelo SBERT\n","modelo = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Dataset de texto\n","texto = [\"Estoy muy feliz\", \"Esto es muy frustrante\", \"Me siento triste\"]\n","\n","# Definir emociones base\n","emociones_base = {\n","    \"alegría\": \"Esto me hace muy feliz y emocionado.\",\n","    \"tristeza\": \"Siento una profunda tristeza y desolación.\",\n","    \"enojo\": \"Estoy molesto y frustrado.\"\n","}\n","\n","# Generar embeddings para textos y emociones base\n","emb_textos = modelo.encode(texto)\n","emb_emociones = modelo.encode(list(emociones_base.values()))\n","\n","# Calcular similitud\n","for i, t in enumerate(texto):\n","    similitudes = cosine_similarity([emb_textos[i]], emb_emociones)\n","    emocion_predicha = list(emociones_base.keys())[np.argmax(similitudes)]\n","    print(f\"Texto: {t}\\nEmoción: {emocion_predicha}\\n\")"],"metadata":{"id":"nvYIk1rFf4Aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# Cargar el modelo SBERT\n","modelo = SentenceTransformer(\"all-MiniLM-L6-v2\")"],"metadata":{"id":"b95CPu48f5Gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto = bd['text'].tolist()"],"metadata":{"id":"WRsckzGsf5xZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definir emociones base\n","emociones_base = {\n","    \"alegría\": \"Esto me hace muy feliz y emocionado.\",\n","    \"tristeza\": \"Siento una profunda tristeza y desolación.\",\n","    \"enojo\": \"Estoy molesto y frustrado.\"\n","}\n","\n","# Generar embeddings para textos y emociones base\n","emb_textos = modelo.encode(texto)\n","emb_emociones = modelo.encode(list(emociones_base.values()))\n","\n","# Calcular similitud\n","for i, t in enumerate(texto):\n","    similitudes = cosine_similarity([emb_textos[i]], emb_emociones)\n","    emocion_predicha = list(emociones_base.keys())[np.argmax(similitudes)]\n","    print(f\"Texto: {t}\\nEmoción: {emocion_predicha}\\n\")"],"metadata":{"id":"ocoNQvGnf6yC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.cluster import KMeans\n","import pandas as pd\n","\n","# Cargar el modelo LaBSE\n","modelo = SentenceTransformer(\"sentence-transformers/LaBSE\")\n","\n","# Generar vectores\n","vectores = modelo.encode(bd['text'].tolist())\n","\n","# Agrupar con KMeans\n","num_clusters = 3\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n","clusters = kmeans.fit_predict(vectores)\n","\n","# Añadir resultados al DataFrame\n","bd[\"cluster\"] = clusters"],"metadata":{"id":"fIsZs7sFf8FJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd.head(10)"],"metadata":{"id":"f3qHW3Lzf9IC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['cluster'].value_counts()"],"metadata":{"id":"YfXxWpVwf-Oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función de limpieza de texto\n","def limpiar_texto(text):\n","    text = re.sub(r'http\\S+', '', text)  # Eliminar URLs\n","    text = re.sub(r'@\\w+', '', text)     # Eliminar menciones\n","    text = re.sub(r'#\\w+', '', text)     # Eliminar hashtags\n","    text = re.sub(r'\\d+', '', text)      # Eliminar números\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar puntuaciones\n","    text = text.lower()                  # Convertir a minúsculas\n","    return text\n","\n","# Aplicar la limpieza a la columna de tuits\n","bd['texto_limpio2'] = bd['text'].apply(limpiar_texto)"],"metadata":{"id":"BqO5WqQ0f_W6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto = bd['texto_limpio2'].tolist()"],"metadata":{"id":"BB3lDSxsgAHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Modelo preentrenado SBERT"],"metadata":{"id":"-R6r1LxtgBD5"}},{"cell_type":"code","source":["# Definir emociones base\n","emociones_base = {\n","    \"positivo\": ['Que estilo', 'La tienda más cool del mundo', 'Me merecen esos jogger',  'lo hacen muy bien', 'bonita la ropa', 'me encanta su contenido', 'La mejor ropa','Belleza', 'Wow 😍❤️', 'Me encanta el 3','Últimamente me gusta sus prendas ','Eso si es verdad, excelente calidad en las telas, amo las camisas blancas de ustedes, tengo 3 ','La calidad de las telas si ha mejorado muchísimo. 😍😍😍😍😍','Amo ese vestido blanco lo veo y lo quiero 😍'],\n","    \"neutro\": ['precio por favor',  'el top está en la tienda','item de la camiseta', 'lo tienen en laureles medellín talla s' , 'por el sitio web se pueden comprar?',\n","               'donde están ubicados', 'que item tiene el pantalon'],\n","    \"negativo\": ['Hagan tallas más pequeñas',  'la otra vez  no habían', 'Jamás los dejan entrar', 'Traten de que hagan talla grandes', 'la ropa esta cara',\n","                  'la pagina no deja comprar', 'la ropa esta fea','Buenas tardes realice un pedido y me lo enviaron por TCC pero mirando la guía dice que llevan 2 intentos y no encuentran mi dirección',\n","                  'Cuando algo que no sea oversize ? 😢','Se les acabaron las ideas? 🥴','saquen otra vez vestiditos sueltos como los de hace años ,hacen falta 😢',\n","                  'Tuve un problema con una compra me gustaría resolverlo',\n","                  'Creo que sus camisetas y sus últimas publicaciones acerca de la \"friendzone\" atentan terriblemente contra el derecho de decir NO de una mujer']\n","\n","}\n","\n","# Generar embeddings para emociones base\n","emociones_textos = [item for sublist in emociones_base.values() for item in sublist]  # Aplanar la lista de emociones\n","emb_emociones = modelo.encode(emociones_textos)\n","\n","# Crear un diccionario de emociones con índices\n","emocion_indices = list(emociones_base.keys())\n","emocion_dict = {}\n","index = 0\n","for emocion, frases in emociones_base.items():\n","    emocion_dict[emocion] = list(range(index, index + len(frases)))\n","    index += len(frases)\n","\n","def predecir_emocion(texto):\n","    try:\n","        emb_texto = modelo.encode([texto])\n","        similitudes = cosine_similarity(emb_texto, emb_emociones)\n","        max_index = np.argmax(similitudes)  # Obtener el índice de la máxima similitud\n","        emocion_predicha = None\n","        for emocion, indices in emocion_dict.items():\n","            if max_index in indices:\n","                emocion_predicha = emocion\n","                break\n","        return emocion_predicha\n","    except Exception as e:\n","        print(f\"Error procesando el texto: {texto}, {e}\")\n","        return None\n","\n","# Aplicar al dataframe\n","bd['emocion'] = bd['texto_limpio2'].apply(predecir_emocion)"],"metadata":{"id":"m3HDyqJBgCyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd['emocion'].value_counts()"],"metadata":{"id":"R_UIuRqrgDnp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd[bd['emocion'] == 'negativo'].tail(10)"],"metadata":{"id":"olSAqftMgEdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cargar el modelo LaBSE\n","modelo = SentenceTransformer(\"sentence-transformers/LaBSE\")\n","\n","# Generar vectores\n","vectores = modelo.encode(bd['texto_limpio2'].tolist())\n","\n","# Agrupar con KMeans\n","num_clusters = 3\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n","clusters = kmeans.fit_predict(vectores)\n","\n","# Añadir resultados al DataFrame\n","bd[\"cluster\"] = clusters"],"metadata":{"id":"Q3TkLcYvgFXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bd[bd['cluster'] == 0].tail(10)"],"metadata":{"id":"kCENNuhUgGa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función de limpieza que conserva emojis\n","def limpiar_texto_con_emojis(text):\n","    text = re.sub(r'http\\S+', '', text)  # Eliminar URLs\n","    text = re.sub(r'@\\w+', '', text)     # Eliminar menciones\n","    text = re.sub(r'#\\w+', '', text)     # Eliminar hashtags\n","    text = re.sub(r'\\d+', '', text)      # Eliminar números\n","    text = re.sub(r'[^\\w\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF]', '', text)\n","    text = text.lower()                  # Convertir a minúsculas\n","    return text\n","\n","bd['texto_limpio3'] = bd['text'].apply(limpiar_texto)"],"metadata":{"id":"2SCBZxfMgHkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generar vectores\n","vectores = modelo.encode(bd['texto_limpio3'].tolist())\n","\n","# Agrupar con KMeans\n","num_clusters = 3\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n","clusters = kmeans.fit_predict(vectores)\n","\n","# Añadir resultados al DataFrame\n","bd[\"cluster\"] = clusters"],"metadata":{"id":"dJaUy6twgIgJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","\n","# Cargar un modelo compatible\n","modelo_beto_sbert = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n","\n","print(\"Modelo BETO-SBERT alternativo cargado con éxito.\")"],"metadata":{"id":"CJp2F3-9gKBS"},"execution_count":null,"outputs":[]}]}